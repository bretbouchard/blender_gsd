# Phase 7.5 Research: Advanced Features

**Phase**: 7.5 - Advanced Features
**Research Date**: 2026-02-19
**Researcher**: GSD Phase Researcher

---

## Executive Summary

Phase 7.5 introduces four major advanced feature categories for the Cinematic System:

1. **REQ-TRACK-OBJECT**: Planar tracking, rigid body tracking, knob/fader rotation extraction
2. **REQ-TRACK-SCAN**: LiDAR/Scan import with floor plane and scale detection
3. **REQ-TRACK-MOCAP**: Motion capture import and retargeting to morphing engine
4. **REQ-TRACK-BATCH**: Multi-shot batch processing with parallel execution

This research document provides technical guidance for planning these features effectively.

---

## 1. Blender's Planar Tracking API

### 1.1 Core API Components

Blender provides planar tracking through the `MovieTrackingPlaneTrack` class:

**Key Properties:**
- `markers`: Collection of `MovieTrackingPlaneMarker` objects
- `corners`: Four corner coordinates defining the plane (normalized 0-1)
- `image`: Optional image/sequence displayed in the plane
- `image_opacity`: Opacity of the displayed image (0-1)
- `name`: Plane track identifier

**Marker Structure:**
Each marker contains:
- `frame`: Frame number
- `corners`: Four 2D corner positions at that frame
- `enabled`: Whether the marker is active

### 1.2 Compositor Integration

Blender provides two compositor nodes for planar tracking:

1. **`CompositorNodeCornerPin`**
   - Uses plane track corners for perspective transform
   - Maps four input corners to four output corners
   - Useful for screen replacements and perspective matching

2. **`CompositorNodePlaneTrackDeform`**
   - Deforms input image to match plane track perspective
   - Tracks plane movement across frames
   - Enables content insertion into tracked planes

### 1.3 API Access Pattern

```python
import bpy

# Access movie clip
clip = bpy.data.movieclips["footage.mp4"]

# Get tracking object
tracking = clip.tracking

# Access plane tracks
plane_tracks = tracking.plane_tracks

# Get specific plane track
plane_track = plane_tracks["Plane"]

# Access corners at current frame
corners = plane_track.corners  # [(x1,y1), (x2,y2), (x3,y3), (x4,y4)]

# Iterate markers
for marker in plane_track.markers:
    frame = marker.frame
    frame_corners = marker.corners
```

### 1.4 Implementation Recommendations

**For REQ-TRACK-OBJECT:**
- Use `MovieTrackingPlaneTrack` for tracking flat surfaces (mixer panels, console faces)
- Extract corner positions per frame for perspective-aware tracking
- Integrate with existing `Track` dataclass in `types.py`

**Data Mapping:**
```
MovieTrackingPlaneMarker.corners -> Track.custom_params["corners"]
MovieTrackingPlaneMarker.frame -> TrackPoint.frame
```

---

## 2. LiDAR Scan Import and Floor Plane Detection

### 2.1 Supported Scan Formats

**Mobile Scanning Apps:**
- **Polycam**: Exports as `.ply`, `.obj`, `.gltf`, `.e57`
- **RealityScan**: Exports as `.obj`, `.fbx`
- **3D Scanner App**: Exports as `.ply`, `.obj`, `.stl`
- **SiteScape**: Exports as `.e57`, `.las`

**Recommended Import Format:**
- `.ply` (Stanford PLY) - Most universally supported
- `.obj` - Widely compatible, includes materials
- `.gltf/.glb` - Modern, includes PBR materials

### 2.2 Floor Plane Detection Algorithm

**RANSAC-based Plane Fitting:**

```python
import numpy as np
from sklearn.linear_model import RANSACRegressor

def detect_floor_plane(points: np.ndarray, threshold: float = 0.01):
    """
    Detect floor plane from point cloud using RANSAC.

    Args:
        points: Nx3 array of 3D points
        threshold: Inlier threshold in meters

    Returns:
        plane_normal: (a, b, c) normalized plane normal
        plane_d: Plane equation offset (ax + by + cz + d = 0)
        inlier_ratio: Percentage of points on floor
    """
    # Use horizontal component for floor detection
    # Floor is typically the largest horizontal plane

    # Fit plane using RANSAC
    X = points[:, :2]  # x, y
    y = points[:, 2]   # z (height)

    ransac = RANSACRegressor(residual_threshold=threshold)
    ransac.fit(X, y)

    # Plane equation: z = a*x + b*y + c
    a, b = ransac.estimator_.coef_
    c = ransac.estimator_.intercept_

    # Convert to normal form
    normal = np.array([-a, -b, 1.0])
    normal = normal / np.linalg.norm(normal)
    d = -c / np.linalg.norm(np.array([-a, -b, 1.0]))

    inlier_ratio = np.mean(ransac.inlier_mask_)

    return normal, d, inlier_ratio
```

**Alternative with Open3D:**
```python
import open3d as o3d

def detect_floor_open3d(pcd: o3d.geometry.PointCloud):
    """Detect floor using Open3D's RANSAC plane segmentation."""
    plane_model, inliers = pcd.segment_plane(
        distance_threshold=0.01,
        ransac_n=3,
        num_iterations=1000
    )
    # plane_model = [a, b, c, d] where ax + by + cz + d = 0
    return plane_model, inliers
```

### 2.3 Scale Detection Strategies

**Method 1: Known Object Detection**
- Detect known-sized objects in scan (markers, calibration objects)
- Calculate scale factor from measured vs. expected size

**Method 2: ArUco Marker Integration**
```python
import cv2

def detect_aruco_scale(image, marker_size_meters=0.1):
    """Detect ArUco markers and calculate scene scale."""
    dictionary = cv2.aruco.getPredefinedDictionary(cv2.aruco.DICT_4X4_50)
    corners, ids, _ = cv2.aruco.detectMarkers(image, dictionary)

    if ids is not None:
        # Get marker size in pixels
        marker_size_px = np.mean([np.linalg.norm(c[0][0] - c[0][2]) for c in corners])
        scale = marker_size_meters / marker_size_px
        return scale
    return None
```

**Method 3: Statistical Analysis**
- Assume typical ceiling height (2.4-3.0m)
- Detect ceiling plane and floor plane
- Calculate scale from height difference

### 2.4 Implementation Recommendations

**New Dataclasses Needed:**

```python
@dataclass
class ScanData:
    """Imported scan data."""
    source_path: str
    format: str  # "ply", "obj", "gltf", "e57"
    points: np.ndarray  # Nx3 point cloud
    mesh: Optional[Any]  # Blender mesh object
    floor_plane: Optional[Tuple[float, float, float, float]]  # (a, b, c, d)
    scale_factor: float = 1.0
    confidence: float = 0.0

@dataclass
class FloorDetectionResult:
    """Result of floor plane detection."""
    plane_normal: Tuple[float, float, float]
    plane_d: float
    inlier_ratio: float
    scale_estimate: float
    confidence: float
```

---

## 3. BVH/FBX Mocap Import and Retargeting

### 3.1 Existing Infrastructure

The codebase already has BVH and FBX parsers in `lib/cinematic/tracking/import_export.py`:

**BVHParser (lines ~350-450):**
- Parses BVH hierarchy
- Extracts joint positions and rotations
- Returns `ImportedCamera` dataclass

**FBXParser (lines ~200-350):**
- Parses FBX camera data
- Extracts animation curves
- Handles transformation matrices

### 3.2 Mocap Data Sources

**Move.ai:**
- Exports: FBX, BVH
- Features: Full body tracking, hand tracking
- Quality: Production-ready

**Rokoko:**
- Exports: FBX, BVH
- Features: Real-time streaming, finger tracking
- Hardware: Rokoko Smartsuit

**MediaPipe (Free Alternative):**
- Hand pose estimation from video
- 21 hand landmarks
- Python library available

### 3.3 Retargeting to Morphing Engine

The `MorphingEngine` in `lib/control_system/morphing.py` provides the foundation:

**Key Integration Points:**

```python
# MorphTarget can be created from mocap data
@classmethod
def from_mocap(cls, mocap_data: dict) -> "MorphTarget":
    """Create morph target from motion capture data."""
    # Extract hand position/orientation
    # Map to geometry parameters
    # Create interpolated target
    pass

# MorphAnimation can be driven by mocap timeline
class MocapMorphAnimation(MorphAnimation):
    """Animation driven by mocap data."""
    mocap_source: str
    mocap_fps: float

    def evaluate(self, time: float) -> float:
        """Evaluate morph from mocap frame data."""
        # Get mocap frame at time
        # Calculate morph factor from joint positions
        return self._mocap_to_factor(time)
```

### 3.4 Hand Animation for Control Surfaces

**Approach:**
1. Import mocap hand data (21 landmarks from MediaPipe or 15 from Rokoko)
2. Calculate finger joint angles
3. Map finger positions to control surface parameters:
   - Index finger X/Y -> Knob rotation (0-270 degrees)
   - All fingers extended -> Fader position (0-1)
   - Pinch gesture -> Button press

**Implementation:**

```python
@dataclass
class HandTrackingData:
    """Hand tracking data for control surfaces."""
    frame: int
    landmarks: List[Tuple[float, float, float]]  # 21 MediaPipe landmarks
    confidence: float

    def get_knob_rotation(self, knob_center: Tuple[float, float]) -> float:
        """Calculate knob rotation from index finger position."""
        # Get index finger tip (landmark 8)
        index_tip = self.landmarks[8]

        # Calculate angle from center
        dx = index_tip[0] - knob_center[0]
        dy = index_tip[1] - knob_center[1]
        angle = math.atan2(dy, dx)

        # Convert to knob rotation (0-270 degrees)
        rotation = (angle + math.pi) / (2 * math.pi) * 270
        return rotation

    def get_fader_position(self, fader_bounds: Tuple[float, float, float, float]) -> float:
        """Calculate fader position from finger Y coordinate."""
        # Get index finger base (landmark 5)
        index_base = self.landmarks[5]

        # Normalize within fader bounds
        y_min, y_max = fader_bounds[1], fader_bounds[3]
        position = (index_base[1] - y_min) / (y_max - y_min)
        return max(0.0, min(1.0, position))
```

### 3.5 Implementation Recommendations

**New Module: `lib/cinematic/tracking/mocap.py`**

```python
"""Motion capture import and retargeting system."""

class MocapImporter:
    """Import mocap data from various sources."""

    def import_move_ai(self, path: str) -> MocapData:
        """Import from Move.ai FBX/BVH."""
        pass

    def import_rokoko(self, path: str) -> MocapData:
        """Import from Rokoko Studio."""
        pass

    def import_mediapipe(self, video_path: str) -> HandTrackingData:
        """Process video with MediaPipe hand tracking."""
        pass

class MocapRetargeter:
    """Retarget mocap to control surfaces."""

    def retarget_to_knob(self, data: HandTrackingData, knob_id: str) -> KnobAnimation:
        """Convert hand tracking to knob rotation."""
        pass

    def retarget_to_fader(self, data: HandTrackingData, fader_id: str) -> FaderAnimation:
        """Convert hand tracking to fader position."""
        pass

    def retarget_to_morph(self, data: MocapData, target: MorphTarget) -> MorphAnimation:
        """Create morph animation from mocap."""
        pass
```

---

## 4. Batch Processing with Parallel Execution

### 4.1 Python Parallel Processing Patterns

**Multiprocessing Pool:**
```python
from multiprocessing import Pool, cpu_count

def render_shot(shot_config: dict) -> RenderResult:
    """Render a single shot."""
    # Setup scene
    # Render
    # Return result
    pass

def batch_render(shot_configs: List[dict], workers: int = None):
    """Parallel batch rendering."""
    workers = workers or cpu_count()

    with Pool(workers) as pool:
        results = pool.map(render_shot, shot_configs)

    return results
```

**Concurrent Futures (More Control):**
```python
from concurrent.futures import ProcessPoolExecutor, as_completed

def batch_render_with_progress(shot_configs: List[dict], workers: int = 4):
    """Batch rendering with progress tracking."""
    results = []
    failures = []

    with ProcessPoolExecutor(max_workers=workers) as executor:
        futures = {
            executor.submit(render_shot, config): config
            for config in shot_configs
        }

        for future in as_completed(futures):
            config = futures[future]
            try:
                result = future.result()
                results.append(result)
            except Exception as e:
                failures.append((config, str(e)))

    return results, failures
```

### 4.2 Resume on Failure Pattern

```python
import json
from pathlib import Path

class BatchProcessor:
    """Batch processor with resume capability."""

    def __init__(self, checkpoint_path: str = ".batch_checkpoint.json"):
        self.checkpoint_path = Path(checkpoint_path)
        self.completed = self._load_checkpoint()

    def _load_checkpoint(self) -> set:
        """Load completed task IDs."""
        if self.checkpoint_path.exists():
            with open(self.checkpoint_path) as f:
                return set(json.load(f).get("completed", []))
        return set()

    def _save_checkpoint(self):
        """Save checkpoint."""
        with open(self.checkpoint_path, 'w') as f:
            json.dump({"completed": list(self.completed)}, f)

    def process_batch(self, tasks: List[dict], workers: int = 4):
        """Process batch with resume on failure."""
        remaining = [t for t in tasks if t["id"] not in self.completed]

        with ProcessPoolExecutor(max_workers=workers) as executor:
            futures = {
                executor.submit(self._process_task, task): task
                for task in remaining
            }

            for future in as_completed(futures):
                task = futures[future]
                try:
                    future.result()
                    self.completed.add(task["id"])
                    self._save_checkpoint()
                except Exception as e:
                    print(f"Task {task['id']} failed: {e}")
                    # Continue with other tasks

    def _process_task(self, task: dict):
        """Process single task."""
        pass
```

### 4.3 Blender-Specific Considerations

**Blender is NOT thread-safe within a single process.**

Options for parallel Blender rendering:

1. **Multiple Blender Instances:**
```python
import subprocess

def render_blender_scene(blend_file: str, output: str, frame: int):
    """Render single frame in separate Blender process."""
    cmd = [
        "blender",
        "-b", blend_file,
        "-o", output,
        "-f", str(frame)
    ]
    subprocess.run(cmd, check=True)

def parallel_render(blend_file: str, frames: List[int], workers: int = 4):
    """Parallel frame rendering with multiple Blender instances."""
    with ProcessPoolExecutor(max_workers=workers) as executor:
        futures = [
            executor.submit(
                render_blender_scene,
                blend_file,
                f"//output/frame_{f:04d}_",
                f
            )
            for f in frames
        ]
        # Wait for completion
```

2. **Blender Command-Line Farm:**
```python
def render_farm(shot_configs: List[dict]):
    """Distribute rendering across multiple Blender instances."""
    for i, config in enumerate(shot_configs):
        # Create temporary .blend for this shot
        temp_blend = create_shot_blend(config)

        # Launch Blender in background
        subprocess.Popen([
            "blender", "-b", temp_blend,
            "-o", config["output_path"],
            "-a"  # Render animation
        ])
```

### 4.4 Implementation Recommendations

**New Module: `lib/cinematic/batch.py`**

```python
"""Batch processing system for cinematic rendering."""

@dataclass
class BatchJob:
    """Single batch job definition."""
    id: str
    shot_config: dict
    output_path: str
    status: str = "pending"  # pending, running, completed, failed
    error: Optional[str] = None

@dataclass
class BatchConfig:
    """Batch processing configuration."""
    workers: int = 4
    resume_on_failure: bool = True
    checkpoint_path: str = ".batch_checkpoint.json"
    max_retries: int = 3

class BatchProcessor:
    """Main batch processor with parallel execution."""

    def __init__(self, config: BatchConfig):
        self.config = config
        self.checkpoint = BatchCheckpoint(config.checkpoint_path)

    def submit_batch(self, jobs: List[BatchJob]) -> BatchResult:
        """Submit and process batch jobs."""
        pass

    def resume_batch(self) -> BatchResult:
        """Resume incomplete batch."""
        pass

    def cancel_batch(self):
        """Cancel running batch."""
        pass

    def get_progress(self) -> BatchProgress:
        """Get current progress."""
        pass
```

---

## 5. Knob Rotation Extraction from Video

### 5.1 Computer Vision Approaches

**Method 1: Template Matching**
```python
import cv2
import numpy as np

def track_knob_rotation(video_path: str, initial_roi: Tuple[int, int, int, int]):
    """Track knob rotation using template matching."""
    cap = cv2.VideoCapture(video_path)

    # Get initial template
    cap.set(cv2.CAP_PROP_POS_FRAMES, 0)
    ret, frame = cap.read()
    x, y, w, h = initial_roi
    template = frame[y:y+h, x:x+w]

    rotations = []

    while cap.isOpened():
        ret, frame = cap.read()
        if not ret:
            break

        # Template matching
        result = cv2.matchTemplate(frame, template, cv2.TM_CCOEFF_NORMED)
        _, max_val, _, max_loc = cv2.minMaxLoc(result)

        # Calculate rotation from position change
        # (Works for knobs on circular path)
        center_x = x + w // 2
        center_y = y + h // 2

        dx = max_loc[0] + w // 2 - center_x
        dy = max_loc[1] + h // 2 - center_y

        angle = np.arctan2(dy, dx)
        degrees = np.degrees(angle)

        rotations.append(degrees)

    return rotations
```

**Method 2: Hough Circle Detection + Indicator Tracking**
```python
def track_knob_with_indicator(video_path: str):
    """Track knob by detecting circle and indicator line."""
    cap = cv2.VideoCapture(video_path)
    rotations = []

    while cap.isOpened():
        ret, frame = cap.read()
        if not ret:
            break

        gray = cv2.cvtColor(frame, cv2.COLOR_BGR2GRAY)

        # Detect knob circle
        circles = cv2.HoughCircles(
            gray,
            cv2.HOUGH_GRADIENT,
            dp=1,
            minDist=50,
            param1=50,
            param2=30,
            minRadius=20,
            maxRadius=100
        )

        if circles is not None:
            circle = circles[0][0]
            cx, cy, r = circle

            # Extract ROI around knob
            roi = frame[int(cy-r):int(cy+r), int(cx-r):int(cx+r)]

            # Detect indicator line using edge detection
            edges = cv2.Canny(roi, 50, 150)
            lines = cv2.HoughLines(edges, 1, np.pi/180, 50)

            if lines is not None:
                # Get dominant line angle
                angles = [line[0][1] for line in lines]
                rotation = np.mean(angles)
                rotations.append(np.degrees(rotation))

    return rotations
```

**Method 3: Optical Flow (Best for Smooth Motion)**
```python
def track_knob_optical_flow(video_path: str, roi: Tuple[int, int, int, int]):
    """Track knob using optical flow."""
    cap = cv2.VideoCapture(video_path)

    # Parameters for Lucas-Kanade optical flow
    lk_params = dict(
        winSize=(15, 15),
        maxLevel=2,
        criteria=(cv2.TERM_CRITERIA_EPS | cv2.TERM_CRITERIA_COUNT, 10, 0.03)
    )

    ret, old_frame = cap.read()
    old_gray = cv2.cvtColor(old_frame, cv2.COLOR_BGR2GRAY)

    # Get points on knob perimeter
    x, y, w, h = roi
    cx, cy = x + w // 2, y + h // 2
    radius = min(w, h) // 2

    # Generate points on circle perimeter
    angles = np.linspace(0, 2 * np.pi, 16, endpoint=False)
    p0 = np.array([
        [cx + radius * np.cos(a), cy + radius * np.sin(a)]
        for a in angles
    ], dtype=np.float32).reshape(-1, 1, 2)

    rotations = [0.0]  # Start at 0

    while cap.isOpened():
        ret, frame = cap.read()
        if not ret:
            break

        frame_gray = cv2.cvtColor(frame, cv2.COLOR_BGR2GRAY)

        # Calculate optical flow
        p1, st, err = cv2.calcOpticalFlowPyrLK(old_gray, frame_gray, p0, None, **lk_params)

        if p1 is not None and st.sum() > 8:
            # Calculate rotation from point movement
            good_new = p1[st == 1]
            good_old = p0[st == 1]

            # Calculate average rotation
            rotation = calculate_rotation_from_points(good_old, good_new, (cx, cy))
            rotations.append(rotations[-1] + rotation)

            p0 = good_new.reshape(-1, 1, 2)

        old_gray = frame_gray.copy()

    return rotations[1:]  # Skip initial 0

def calculate_rotation_from_points(old_pts, new_pts, center):
    """Calculate rotation angle from point movement around center."""
    cx, cy = center

    angles_old = np.arctan2(old_pts[:, 1] - cy, old_pts[:, 0] - cx)
    angles_new = np.arctan2(new_pts[:, 1] - cy, new_pts[:, 0] - cx)

    # Handle angle wrapping
    angle_diffs = angles_new - angles_old
    angle_diffs = np.arctan2(np.sin(angle_diffs), np.cos(angle_diffs))

    return np.degrees(np.mean(angle_diffs))
```

### 5.2 Integration with Existing KLT Tracker

The codebase already has `KLTTracker` in `lib/cinematic/tracking/point_tracker.py`. Extend it:

```python
class KnobRotationTracker:
    """Track knob rotation using existing KLT infrastructure."""

    def __init__(self, klt_tracker: KLTTracker):
        self.klt = klt_tracker
        self.knob_centers: Dict[str, Tuple[float, float]] = {}

    def add_knob(self, knob_id: str, center: Tuple[float, float], radius: float):
        """Register a knob to track."""
        self.knob_centers[knob_id] = center
        # Add tracking points on perimeter
        for angle in np.linspace(0, 2*np.pi, 16, endpoint=False):
            x = center[0] + radius * np.cos(angle)
            y = center[1] + radius * np.sin(angle)
            self.klt.add_point((x, y))

    def get_rotation(self, knob_id: str, frame: int) -> float:
        """Get rotation at frame for knob."""
        center = self.knob_centers[knob_id]
        # Calculate rotation from tracked points around center
        # ...
```

---

## 6. Integration with Morphing Engine

### 6.1 Current Morphing Engine Capabilities

The `MorphEngine` (`lib/control_system/morphing.py`) provides:

- **MorphTarget**: Complete control surface state (geometry, material, surface)
- **MorphAnimation**: Time-based interpolation between targets
- **Easing Functions**: 28 easing types (LINEAR through EASE_IN_OUT_BOUNCE)
- **StaggeredMorph**: Wave-like animation across multiple controls
- **Color Interpolation**: LAB color space for perceptually uniform blends

### 6.2 Integration Points for Tracking Data

**1. Knob Rotation -> MorphTarget:**
```python
def rotation_to_morph(rotation_degrees: float, range_degrees: float = 270) -> float:
    """Convert knob rotation to morph factor (0-1)."""
    normalized = rotation_degrees / range_degrees
    return max(0.0, min(1.0, normalized))

# Usage
rotation = knob_tracker.get_rotation("gain_knob", frame=100)
morph_factor = rotation_to_morph(rotation, range_degrees=270)

# Apply to morph engine
current_morph = morph_engine.evaluate(animation, time=morph_factor)
```

**2. Fader Position -> MorphTarget:**
```python
def fader_to_morph(fader_y: float, bounds: Tuple[float, float]) -> float:
    """Convert fader Y position to morph factor."""
    y_min, y_max = bounds
    normalized = (fader_y - y_min) / (y_max - y_min)
    return max(0.0, min(1.0, normalized))
```

**3. Mocap Hand -> Control Surface:**
```python
class MocapMorphBridge:
    """Bridge between mocap data and morphing engine."""

    def __init__(self, morph_engine: MorphEngine):
        self.engine = morph_engine
        self.control_mapping = {}

    def map_hand_to_control(self, hand_data: HandTrackingData, control_id: str):
        """Map hand position to control morph factor."""
        # Get control type and bounds
        control_type, bounds = self.control_mapping[control_id]

        if control_type == "knob":
            factor = hand_data.get_knob_rotation(bounds["center"])
        elif control_type == "fader":
            factor = hand_data.get_fader_position(bounds)
        else:
            factor = 0.0

        return factor

    def create_morph_from_mocap(
        self,
        mocap_data: List[HandTrackingData],
        control_id: str,
        source_target: MorphTarget,
        dest_target: MorphTarget
    ) -> MorphAnimation:
        """Create morph animation from mocap sequence."""
        keyframes = []

        for frame_data in mocap_data:
            factor = self.map_hand_to_control(frame_data, control_id)
            keyframes.append(MorphKeyframe(
                time=frame_data.frame / len(mocap_data),
                value=factor,
                easing=EasingType.LINEAR
            ))

        return MorphAnimation(
            source=source_target,
            target=dest_target,
            duration=len(mocap_data) / 24.0,  # Assume 24fps
            keyframes=keyframes
        )
```

### 6.3 Staggered Animation for Console Sections

For batch operations on multiple controls:

```python
def create_staggered_console_morph(
    controls: List[str],
    mocap_data: Dict[str, List[HandTrackingData]],
    base_animation: MorphAnimation
) -> StaggeredMorph:
    """Create staggered morph for console-wide control changes."""
    stagger = StaggeredMorph(
        animation=base_animation,
        control_count=len(controls),
        stagger=StaggerConfig(
            stagger_type="linear",
            stagger_amount=0.05,  # 5% delay between controls
            stagger_direction="forward"
        )
    )
    return stagger
```

---

## 7. Implementation Priority

Based on complexity and dependencies:

### Phase 7.5.1: Batch Processing (Lower Complexity)
- Builds on existing render system
- Python standard library (multiprocessing)
- Immediate value for existing shots
- **Estimated Effort**: 2-3 days

### Phase 7.5.2: Knob Rotation Tracking (Medium Complexity)
- Extends existing KLT tracker
- OpenCV is already a dependency
- Clear integration path to morphing engine
- **Estimated Effort**: 3-4 days

### Phase 7.5.3: LiDAR/Scan Import (Medium Complexity)
- New dependencies (Open3D or scikit-learn)
- Clear algorithmic approach (RANSAC)
- Integration with backdrop system
- **Estimated Effort**: 3-4 days

### Phase 7.5.4: Mocap Integration (Higher Complexity)
- Multiple import formats
- Complex retargeting logic
- Hand tracking integration
- **Estimated Effort**: 5-7 days

---

## 8. Dependencies and Requirements

### Python Packages

**Already in Project:**
- `numpy`: Array operations
- `opencv-python` (implied by KLT tracker): Computer vision
- `dataclasses`: Data structures
- `yaml`: Configuration

**New Dependencies:**

```txt
# For LiDAR processing (choose one):
open3d>=0.17.0           # Full point cloud processing
# OR
scikit-learn>=1.0.0      # RANSAC only

# For mocap processing:
mediapipe>=0.10.0        # Hand tracking (optional, free)
# FBX/BVH parsing already exists

# For advanced batch processing:
psutil>=5.9.0            # Process management
```

### Blender Version Requirements

- Blender 4.0+ for `BLENDER_EEVEE_NEXT`
- Blender 3.6+ for planar tracking compositor nodes
- Python 3.10+ for new type hint syntax

---

## 9. Risk Assessment

### Technical Risks

| Risk | Likelihood | Impact | Mitigation |
|------|------------|--------|------------|
| Blender multiprocessing issues | High | High | Use subprocess isolation, checkpoint system |
| OpenCV memory leaks | Medium | Medium | Process in batches, explicit cleanup |
| Mocap format variations | Medium | Medium | Support core formats, add converters |
| LiDAR point density | Low | Low | Adaptive sampling, LOD system |
| MediaPipe accuracy | Medium | Low | Confidence filtering, manual correction |

### Integration Risks

| Risk | Likelihood | Impact | Mitigation |
|------|------------|--------|------------|
| Morphing engine API changes | Low | High | Interface abstraction layer |
| Tracking data format drift | Medium | Medium | Version migration system |
| Performance with large batches | Medium | Medium | Chunking, streaming processing |

---

## 10. Recommended Plan Structure

### PLAN 7.5.1: Batch Processing Foundation
- Implement `BatchProcessor` class
- Add checkpoint/resume system
- Create batch YAML format
- Integrate with render system

### PLAN 7.5.2: Knob Rotation Tracking
- Extend `KLTTracker` for circular motion
- Implement rotation extraction algorithms
- Add rotation-to-morph bridge
- Create tracking ROI configuration

### PLAN 7.5.3: LiDAR Scan Import
- Add PLY/OBJ point cloud importers
- Implement RANSAC floor detection
- Add scale detection methods
- Integrate with backdrop system

### PLAN 7.5.4: Mocap Integration
- Extend BVH parser for full skeleton
- Add hand tracking importer
- Implement retargeting to morph targets
- Create control mapping system

---

## Appendix A: Key Files to Modify

| File | Changes |
|------|---------|
| `lib/cinematic/tracking/types.py` | Add `ScanData`, `HandTrackingData`, `BatchJob` |
| `lib/cinematic/tracking/point_tracker.py` | Add `KnobRotationTracker` |
| `lib/cinematic/tracking/import_export.py` | Add scan importers |
| `lib/control_system/morphing.py` | Add mocap integration methods |
| New: `lib/cinematic/tracking/mocap.py` | Mocap import and retargeting |
| New: `lib/cinematic/batch.py` | Batch processing system |

---

## Appendix B: Testing Strategy

### Unit Tests
- Rotation extraction algorithms
- Floor plane detection accuracy
- Mocap data parsing
- Batch checkpoint/resume

### Integration Tests
- End-to-end knob tracking to render
- LiDAR scan to scene setup
- Mocap to morph animation
- Multi-shot batch render

### Performance Tests
- Large batch processing (>100 shots)
- Point cloud processing (>1M points)
- Real-time tracking latency

---

**Research Complete**: This document provides the technical foundation for planning Phase 7.5. Focus on batch processing first for immediate value, then progressively add tracking features.
