# Phase 7.2: Footage & Camera Profiles - Research

**Researched:** 2026-02-19
**Domain:** Video metadata extraction, lens distortion modeling, camera calibration
**Confidence:** MEDIUM

## Summary

This phase implements comprehensive footage analysis and camera device profiles for the motion tracking system. The research covers:

1. **FFmpeg/ffprobe** for video metadata extraction - the industry standard for video analysis
2. **iPhone QuickTime metadata** extraction via ffprobe side data and QuickTime atoms
3. **Rolling shutter detection** via frame differencing and motion analysis
4. **Brown-Conrady distortion model** for lens distortion correction
5. **Vanishing point detection** for focal length estimation
6. **ST-Map generation** for compositing workflows

**Primary recommendation:** Extend the existing `FootageImporter` class with ffprobe-based metadata extraction, add a `CameraProfileManager` class for device profiles, and implement distortion/undistortion functions using OpenCV-style coefficients.

## Standard Stack

### Core
| Library | Version | Purpose | Why Standard |
|---------|---------|---------|--------------|
| ffprobe (FFmpeg) | 6.0+ | Video metadata extraction | Industry standard, supports all formats |
| OpenCV | 4.8+ | Distortion/undistortion functions | Proven algorithms, same model as Blender/libmv |
| NumPy | 1.24+ | Array operations for distortion | Efficient matrix math |

### Supporting
| Library | Version | Purpose | When to Use |
|---------|---------|---------|-------------|
| Pillow | 10.0+ | Image sequence header parsing | When OpenCV not available |
| PyYAML | 6.0+ | Camera profile configuration | Profile loading/saving |
| struct | stdlib | Binary file parsing (EXR, DPX headers) | Image sequence metadata |

### Alternatives Considered
| Instead of | Could Use | Tradeoff |
|------------|-----------|----------|
| ffprobe | moviepy | moviepy is slower, adds dependencies; ffprobe is more complete |
| OpenCV | Custom implementation | OpenCV is battle-tested; custom is error-prone |
| Brown-Conrady | Division model | Division model is faster but less accurate for wide-angle lenses |

**Installation:**
```bash
# ffprobe comes with ffmpeg
brew install ffmpeg  # macOS
# apt install ffmpeg  # Linux

# Python dependencies (already in project)
pip install opencv-python-headless numpy pyyaml pillow
```

## Architecture Patterns

### Recommended Project Structure
```
lib/cinematic/tracking/
├── footage.py            # FootageImporter (existing, extend)
├── metadata.py           # NEW: FFprobeMetadataExtractor
├── rolling_shutter.py    # NEW: Rolling shutter detection/compensation
├── lens_distortion.py    # NEW: Brown-Conrady implementation
├── vanishing_points.py   # NEW: Focal length estimation
├── st_map.py             # NEW: ST-Map generation
└── types.py              # CameraProfile, FootageMetadata (extend)

configs/cinematic/tracking/
├── camera_profiles.yaml  # Extended with tracking-specific data
└── rolling_shutter_presets.yaml  # NEW: Sensor read times
```

### Pattern 1: FFprobe Metadata Extraction
**What:** Use ffprobe CLI to extract comprehensive video metadata in JSON format
**When to use:** All footage imports for tracking workflows
**Example:**
```python
# Source: ffprobe documentation
import subprocess
import json

def extract_metadata_ffprobe(video_path: str) -> dict:
    """Extract comprehensive metadata using ffprobe."""
    cmd = [
        "ffprobe",
        "-v", "quiet",
        "-print_format", "json",
        "-show_format",
        "-show_streams",
        "-show_data",  # Include side data (iPhone metadata)
        str(video_path)
    ]
    result = subprocess.run(cmd, capture_output=True, text=True, timeout=30)
    return json.loads(result.stdout)
```

### Pattern 2: iPhone QuickTime Metadata
**What:** Extract camera-specific metadata from iPhone QuickTime MOV files
**When to use:** iPhone footage imports
**Example:**
```python
# iPhone stores metadata in QuickTime atoms
# ffprobe exposes this via side_data and tags

def parse_iphone_metadata(ffprobe_data: dict) -> dict:
    """Extract iPhone camera metadata from ffprobe output."""
    metadata = {}

    # Look in format tags
    format_tags = ffprobe_data.get("format", {}).get("tags", {})

    # Common iPhone metadata keys
    metadata["camera_model"] = format_tags.get("com.apple.quicktime.model", "")
    metadata["make"] = format_tags.get("com.apple.quicktime.make", "Apple")
    metadata["software"] = format_tags.get("com.apple.quicktime.software", "")
    metadata["creation_time"] = format_tags.get("creation_time", "")

    # Video stream may have focal length, ISO, aperture
    for stream in ffprobe_data.get("streams", []):
        if stream.get("codec_type") == "video":
            tags = stream.get("tags", {})
            # These may be in different keys depending on iOS version
            metadata["focal_length"] = tags.get("com.apple.quicktime.focal_length", None)
            metadata["iso"] = tags.get("com.apple.quicktime.iso", None)
            metadata["aperture"] = tags.get("com.apple.quicktime.aperture", None)
            break

    return metadata
```

### Pattern 3: Brown-Conrady Distortion Model
**What:** Standard 5-parameter lens distortion model used by OpenCV and Blender
**When to use:** Camera profile distortion, ST-Map generation
**Example:**
```python
# Source: OpenCV camera calibration model
import numpy as np

def brown_conrady_distort(
    x: np.ndarray,
    y: np.ndarray,
    k1: float, k2: float, k3: float,
    p1: float, p2: float
) -> tuple[np.ndarray, np.ndarray]:
    """
    Apply Brown-Conrady distortion to normalized coordinates.

    The model has two components:
    1. Radial distortion (k1, k2, k3) - barrel/pincushion
    2. Tangential distortion (p1, p2) - decentering

    Parameters:
        x, y: Normalized coordinates (x = (u - cx)/fx, y = (v - cy)/fy)
        k1, k2, k3: Radial distortion coefficients
        p1, p2: Tangential distortion coefficients

    Returns:
        x_distorted, y_distorted
    """
    r2 = x**2 + y**2
    r4 = r2**2
    r6 = r2**3

    # Radial distortion factor
    radial = 1 + k1 * r2 + k2 * r4 + k3 * r6

    # Tangential distortion
    dx_tangential = 2 * p1 * x * y + p2 * (r2 + 2 * x**2)
    dy_tangential = p1 * (r2 + 2 * y**2) + 2 * p2 * x * y

    x_distorted = x * radial + dx_tangential
    y_distorted = y * radial + dy_tangential

    return x_distorted, y_distorted


def brown_conrady_undistort(
    x_dist: np.ndarray,
    y_dist: np.ndarray,
    k1: float, k2: float, k3: float,
    p1: float, p2: float,
    iterations: int = 5
) -> tuple[np.ndarray, np.ndarray]:
    """
    Iteratively undistort coordinates (inverse mapping).

    Uses fixed-point iteration to find the undistorted coordinates
    that would produce the given distorted coordinates.
    """
    x = x_dist.copy()
    y = y_dist.copy()

    for _ in range(iterations):
        r2 = x**2 + y**2
        r4 = r2**2
        r6 = r2**3

        radial = 1 + k1 * r2 + k2 * r4 + k3 * r6

        dx_tangential = 2 * p1 * x * y + p2 * (r2 + 2 * x**2)
        dy_tangential = p1 * (r2 + 2 * y**2) + 2 * p2 * x * y

        # Inverse approximation
        x = (x_dist - dx_tangential) / radial
        y = (y_dist - dy_tangential) / radial

    return x, y
```

### Pattern 4: Rolling Shutter Detection
**What:** Detect rolling shutter skew from motion patterns in video
**When to use:** Analysis of handheld/action camera footage
**Example:**
```python
import numpy as np
import cv2

def detect_rolling_shutter(
    video_path: str,
    sample_frames: int = 30
) -> dict:
    """
    Detect rolling shutter artifacts by analyzing row-wise motion.

    Rolling shutter causes each row to be captured at a different time,
    creating skew when there's horizontal camera motion.

    Returns:
        dict with 'detected', 'skew_angle', 'severity' keys
    """
    cap = cv2.VideoCapture(str(video_path))
    if not cap.isOpened():
        return {"detected": False, "error": "Cannot open video"}

    # Sample frames throughout video
    total_frames = int(cap.get(cv2.CAP_PROP_FRAME_COUNT))
    frame_indices = np.linspace(0, total_frames - 1, sample_frames, dtype=int)

    row_displacements = []
    prev_frame = None

    for idx in frame_indices:
        cap.set(cv2.CAP_PROP_POS_FRAMES, idx)
        ret, frame = cap.read()
        if not ret:
            continue

        gray = cv2.cvtColor(frame, cv2.COLOR_BGR2GRAY)

        if prev_frame is not None:
            # Calculate optical flow per row
            height = gray.shape[0]
            num_rows = 10  # Sample 10 horizontal strips
            row_height = height // num_rows

            row_motions = []
            for i in range(num_rows):
                y1 = i * row_height
                y2 = (i + 1) * row_height

                # Track features in this strip
                strip_prev = prev_frame[y1:y2, :]
                strip_curr = gray[y1:y2, :]

                # Calculate mean horizontal displacement
                flow = cv2.calcOpticalFlowFarneback(
                    strip_prev, strip_curr, None,
                    pyr_scale=0.5, levels=3, winsize=15,
                    iterations=3, poly_n=5, poly_sigma=1.2, flags=0
                )
                mean_dx = np.mean(flow[:, :, 0])
                row_motions.append(mean_dx)

            # Check for systematic row-to-row variation
            row_displacements.append(row_motions)

        prev_frame = gray

    cap.release()

    if not row_displacements:
        return {"detected": False, "error": "Insufficient frames"}

    # Analyze displacement patterns
    displacements = np.array(row_displacements)

    # Rolling shutter causes linear relationship between row and displacement
    # Fit a line to row vs displacement
    row_indices = np.arange(displacements.shape[1])
    slopes = []
    for frame_disps in displacements:
        if np.std(frame_disps) > 0.1:  # Only analyze frames with motion
            slope, _ = np.polyfit(row_indices, frame_disps, 1)
            slopes.append(slope)

    if not slopes:
        return {"detected": False, "severity": "none"}

    mean_slope = np.mean(np.abs(slopes))

    # Heuristic thresholds
    if mean_slope > 0.5:
        return {"detected": True, "severity": "high", "skew_per_row": mean_slope}
    elif mean_slope > 0.2:
        return {"detected": True, "severity": "medium", "skew_per_row": mean_slope}
    elif mean_slope > 0.05:
        return {"detected": True, "severity": "low", "skew_per_row": mean_slope}
    else:
        return {"detected": False, "severity": "none", "skew_per_row": mean_slope}
```

### Pattern 5: Vanishing Point Detection for Focal Length
**What:** Estimate focal length from detected vanishing points
**When to use:** When camera profile is unknown
**Example:**
```python
import numpy as np
import cv2

def detect_vanishing_points(image: np.ndarray) -> list[tuple[float, float]]:
    """
    Detect vanishing points from line intersections.

    Uses Canny edge detection and Hough line transform to find lines,
    then clusters intersections to find vanishing points.
    """
    gray = cv2.cvtColor(image, cv2.COLOR_BGR2GRAY) if len(image.shape) == 3 else image

    # Edge detection
    edges = cv2.Canny(gray, 50, 150, apertureSize=3)

    # Line detection
    lines = cv2.HoughLines(edges, 1, np.pi / 180, 100)

    if lines is None or len(lines) < 4:
        return []

    # Convert lines to point-slope form
    line_points = []
    for line in lines:
        rho, theta = line[0]
        a = np.cos(theta)
        b = np.sin(theta)
        x0 = a * rho
        y0 = b * rho
        # Direction vector
        dx = -b
        dy = a
        line_points.append((x0, y0, dx, dy))

    # Find intersections
    intersections = []
    for i, (x1, y1, dx1, dy1) in enumerate(line_points):
        for j, (x2, y2, dx2, dy2) in enumerate(line_points[i+1:], i+1):
            # Line-line intersection
            # x1 + t*dx1 = x2 + s*dx2
            # y1 + t*dy1 = y2 + s*dy2
            det = dx1 * dy2 - dy1 * dx2
            if abs(det) < 1e-6:
                continue  # Parallel lines

            t = ((x2 - x1) * dy2 - (y2 - y1) * dx2) / det
            px = x1 + t * dx1
            py = y1 + t * dy1

            # Only keep reasonable intersections (not too far from image)
            h, w = gray.shape
            if -w < px < 2*w and -h < py < 2*h:
                intersections.append((px, py))

    if not intersections:
        return []

    # Cluster intersections to find vanishing points
    intersections = np.array(intersections)

    # Simple k-means with k=3 (for Manhattan world)
    criteria = (cv2.TERM_CRITERIA_EPS + cv2.TERM_CRITERIA_MAX_ITER, 10, 1.0)
    k = min(3, len(intersections))
    _, labels, centers = cv2.kmeans(
        intersections.astype(np.float32),
        k, None, criteria, 10, cv2.KMEANS_PP_CENTERS
    )

    # Filter to significant clusters
    vanishing_points = []
    for i, center in enumerate(centers):
        cluster_size = np.sum(labels == i)
        if cluster_size >= 3:  # At least 3 intersections
            vanishing_points.append(tuple(center))

    return vanishing_points


def estimate_focal_length_from_vp(
    vanishing_points: list[tuple[float, float]],
    image_width: int,
    image_height: int,
    sensor_width: float = 36.0
) -> float:
    """
    Estimate focal length from two orthogonal vanishing points.

    Based on the orthogonality constraint: if two vanishing points
    correspond to orthogonal directions, the focal length f satisfies:

    f^2 = -((vp1 - c) . (vp2 - c))

    where c is the principal point (usually image center).
    """
    if len(vanishing_points) < 2:
        return 50.0  # Default

    # Principal point assumption (image center)
    cx = image_width / 2
    cy = image_height / 2

    # Try all pairs of vanishing points
    focal_estimates = []

    for i in range(len(vanishing_points)):
        for j in range(i + 1, len(vanishing_points)):
            vp1 = vanishing_points[i]
            vp2 = vanishing_points[j]

            # Translate to principal point
            v1 = np.array([vp1[0] - cx, vp1[1] - cy])
            v2 = np.array([vp2[0] - cx, vp2[1] - cy])

            # For orthogonal directions: v1.v2 = -f^2
            dot_product = np.dot(v1, v2)

            if dot_product < 0:  # Orthogonal directions
                f_pixels = np.sqrt(-dot_product)
                focal_estimates.append(f_pixels)

    if not focal_estimates:
        return 50.0

    # Use median estimate
    f_pixels = np.median(focal_estimates)

    # Convert to 35mm equivalent
    # f_mm = f_pixels * sensor_width / image_width
    f_mm = f_pixels * sensor_width / image_width

    # Clamp to reasonable range
    return max(14.0, min(200.0, f_mm))
```

### Pattern 6: ST-Map Generation
**What:** Generate UV coordinate maps for lens distortion compositing
**When to use:** Compositing workflows that need distortion/undistortion
**Example:**
```python
import numpy as np

def generate_st_map(
    width: int,
    height: int,
    fx: float,
    fy: float,
    cx: float,
    cy: float,
    k1: float, k2: float, k3: float,
    p1: float, p2: float,
    undistort: bool = True
) -> np.ndarray:
    """
    Generate ST-Map (UV coordinate map) for lens distortion.

    ST-Maps are used in compositing to apply lens distortion to footage
    or CG elements. The map encodes where each output pixel should
    sample from in the source image.

    Args:
        width, height: Output resolution
        fx, fy: Focal length in pixels
        cx, cy: Principal point in pixels
        k1, k2, k3: Radial distortion coefficients
        p1, p2: Tangential distortion coefficients
        undistort: If True, map undistorts source; if False, applies distortion

    Returns:
        RGBA image where R=U coordinate, G=V coordinate, B=0, A=1
        Coordinates are normalized 0-1
    """
    # Create coordinate grids
    u = np.arange(width)
    v = np.arange(height)
    u, v = np.meshgrid(u, v)

    # Convert to normalized coordinates
    x = (u - cx) / fx
    y = (v - cy) / fy

    if undistort:
        # Map from distorted to undistorted
        x_undist, y_undist = brown_conrady_undistort(
            x, y, k1, k2, k3, p1, p2
        )

        # Convert back to pixel coordinates
        u_out = x_undist * fx + cx
        v_out = y_undist * fy + cy
    else:
        # Map from undistorted to distorted
        x_dist, y_dist = brown_conrady_distort(
            x, y, k1, k2, k3, p1, p2
        )

        u_out = x_dist * fx + cx
        v_out = y_dist * fy + cy

    # Normalize to 0-1
    u_norm = u_out / width
    v_norm = v_out / height

    # Create EXR-compatible output
    # R = U, G = V, B = 0, A = 1
    st_map = np.zeros((height, width, 4), dtype=np.float32)
    st_map[:, :, 0] = u_norm  # R = U
    st_map[:, :, 1] = v_norm  # G = V
    st_map[:, :, 3] = 1.0     # A = 1

    return st_map


def save_st_map(st_map: np.ndarray, output_path: str):
    """Save ST-Map as EXR file."""
    import cv2
    cv2.imwrite(output_path, st_map)
```

### Anti-Patterns to Avoid

- **Don't use simple radial distortion for wide-angle lenses:** GoPro and ultrawide lenses need full Brown-Conrady model with tangential terms
- **Don't ignore pixel aspect ratio:** DV and anamorphic footage has non-square pixels
- **Don't assume focal length is constant:** Zoom lenses and some primes have slight variation
- **Don't use Python for real-time rolling shutter correction:** It's too slow; pre-compute or use GPU

## Don't Hand-Roll

Problems that look simple but have existing solutions:

| Problem | Don't Build | Use Instead | Why |
|---------|-------------|-------------|-----|
| Video metadata extraction | Custom parser | ffprobe | Handles all edge cases, constantly updated |
| Lens distortion | Custom math | OpenCV cv2.undistort | Battle-tested, handles edge cases |
| Camera calibration | Custom algorithm | OpenCV calibrateCamera | Full bundle adjustment, subpixel accuracy |
| Image sequence detection | Filename parsing regex | Path glob patterns | Handles gaps, padding variations |
| Rolling shutter compensation | Custom warp | Blender Movie Clip Editor | Integrated with tracking |

**Key insight:** Lens distortion and camera calibration are mathematically complex with many edge cases. OpenCV has invested decades into robust implementations. Always prefer OpenCV functions over custom implementations.

## Common Pitfalls

### Pitfall 1: Incorrect Frame Rate Interpretation
**What goes wrong:** Interpreting frame rate as exact decimal when it's a fraction
**Why it happens:** ffprobe returns "30000/1001" for 29.97fps, not 29.97
**How to avoid:** Always parse `r_frame_rate` as fraction, compute float
**Warning signs:** Audio sync drift over long clips
```python
# Correct
fps_str = stream.get("r_frame_rate", "24/1")
if "/" in fps_str:
    num, den = fps_str.split("/")
    fps = float(num) / float(den)
```

### Pitfall 2: Normalized vs Pixel Coordinates
**What goes wrong:** Mixing normalized (0-1) and pixel coordinates
**Why it happens:** Different APIs use different conventions
**How to avoid:** Always document coordinate system, convert at boundaries
**Warning signs:** Distortion appears inverted or scaled incorrectly

### Pitfall 3: Rolling Shutter Detection False Positives
**What goes wrong:** Detecting rolling shutter in footage with object motion
**Why it happens:** Algorithm can't distinguish camera motion from subject motion
**How to avoid:** Use static background regions, require camera motion
**Warning signs:** Static scenes trigger detection

### Pitfall 4: Wrong Distortion Model
**What goes wrong:** Using simple radial for wide-angle or fisheye
**Why it happens:** Different cameras need different models
**How to avoid:** Match model to lens type (Brown-Conrady for rectilinear, fisheye model for fisheye)
**Warning signs:** Undistorted image has curved lines

## Code Examples

### FootageMetadata Dataclass (Extend Existing)
```python
@dataclass
class FootageMetadata:
    """
    Comprehensive footage metadata extracted from video file.

    Extends FootageInfo with content analysis and device metadata.
    """
    # From file (ffprobe)
    source_path: str
    width: int
    height: int
    fps: float
    frame_count: int
    duration_seconds: float
    codec: str
    codec_profile: str = ""
    bit_depth: int = 8
    color_space: str = ""
    color_range: str = "limited"  # limited, full
    pixel_aspect_ratio: float = 1.0

    # Device metadata (iPhone, etc.)
    camera_make: str = ""
    camera_model: str = ""
    lens_model: str = ""
    focal_length_mm: float = 0.0
    iso: int = 0
    aperture: str = ""
    white_balance: int = 0

    # Content analysis
    motion_blur_level: str = "medium"  # low, medium, high
    noise_level: str = "medium"
    contrast_suitability: str = "good"  # poor, fair, good, excellent
    dominant_motion: str = "handheld"  # static, pan, tilt, zoom, handheld

    # Rolling shutter
    rolling_shutter_detected: bool = False
    rolling_shutter_severity: str = "none"  # none, low, medium, high
    rolling_shutter_read_time: float = 0.0  # seconds

    def to_dict(self) -> dict:
        return {
            "source_path": self.source_path,
            "width": self.width,
            "height": self.height,
            "fps": self.fps,
            "frame_count": self.frame_count,
            "duration_seconds": self.duration_seconds,
            "codec": self.codec,
            "codec_profile": self.codec_profile,
            "bit_depth": self.bit_depth,
            "color_space": self.color_space,
            "color_range": self.color_range,
            "pixel_aspect_ratio": self.pixel_aspect_ratio,
            "camera_make": self.camera_make,
            "camera_model": self.camera_model,
            "lens_model": self.lens_model,
            "focal_length_mm": self.focal_length_mm,
            "iso": self.iso,
            "aperture": self.aperture,
            "white_balance": self.white_balance,
            "motion_blur_level": self.motion_blur_level,
            "noise_level": self.noise_level,
            "contrast_suitability": self.contrast_suitability,
            "dominant_motion": self.dominant_motion,
            "rolling_shutter_detected": self.rolling_shutter_detected,
            "rolling_shutter_severity": self.rolling_shutter_severity,
            "rolling_shutter_read_time": self.rolling_shutter_read_time,
        }
```

### Extended Camera Profile (YAML)
```yaml
# configs/cinematic/tracking/camera_profiles.yaml
# Extended with tracking-specific parameters

profiles:
  iphone_15_pro_main:
    name: "iPhone 15 Pro Main Camera"
    sensor_width: 9.8
    sensor_height: 7.3
    focal_length: 6.77  # 24mm equivalent
    crop_factor: 2.94
    aspect_ratio: 1.0

    # Distortion model (from calibration)
    distortion_model: brown_conrady
    distortion_params:
      k1: 0.018
      k2: -0.012
      k3: 0.003
      p1: 0.001
      p2: -0.001

    # Rolling shutter
    rolling_shutter:
      read_time: 0.007  # seconds
      method: row_rolling  # row_rolling, global_shutter

    # Color science
    color_profile: "Apple Log"
    dynamic_range_stops: 13

  gopro_hero12:
    name: "GoPro Hero 12"
    sensor_width: 6.3
    sensor_height: 4.7
    focal_length: 2.9
    crop_factor: 5.7
    aspect_ratio: 1.33

    # Significant barrel distortion
    distortion_model: brown_conrady
    distortion_params:
      k1: -0.25
      k2: 0.08
      k3: 0.0
      p1: 0.0
      p2: 0.0

    # Very fast rolling shutter
    rolling_shutter:
      read_time: 0.016
      method: row_rolling

    color_profile: "Protune Flat"
    dynamic_range_stops: 10
```

## State of the Art

| Old Approach | Current Approach | When Changed | Impact |
|--------------|------------------|--------------|--------|
| Manual calibration grid | Automatic checkerboard detection | OpenCV 3.0 (2015) | Faster, more accurate |
| Single-frame metadata | Per-frame analysis | Industry standard 2020+ | Better tracking accuracy |
| Simple radial distortion | Brown-Conrady with tangential | Blender 2.8+ | Better wide-angle handling |
| Global shutter assumption | Rolling shutter compensation | Industry standard 2015+ | Better mobile footage |

**Deprecated/outdated:**
- **Division model**: Still used for speed but less accurate than Brown-Conrady
- **8-bit only analysis**: Modern footage requires 10-12 bit support

## Open Questions

Things that couldn't be fully resolved:

1. **iPhone 14/15 Pro focal length metadata accuracy**
   - What we know: iPhone embeds 35mm equivalent in QuickTime metadata
   - What's unclear: Exact actual focal length vs equivalent
   - Recommendation: Use profile database for actual values, metadata for verification

2. **Rolling shutter read time by device**
   - What we know: General ranges (7-16ms for phones, 3-5ms for cinema)
   - What's unclear: Exact values for each model, varies with resolution
   - Recommendation: Create preset database with measured values, allow override

3. **Content analysis accuracy without ML**
   - What we know: Basic metrics (blur, noise, contrast) work with classical CV
   - What's unclear: Accuracy of motion classification without neural nets
   - Recommendation: Implement classical methods first, add ML later if needed

## Sources

### Primary (HIGH confidence)
- ffprobe documentation: https://ffmpeg.org/ffprobe.html
- OpenCV camera calibration: https://docs.opencv.org/4.x/dc/dbb/tutorial_py_calibration.html
- Brown, D.C. (1966). "Decentering Distortion of Lenses"
- Existing code: `lib/cinematic/tracking/footage.py`, `lib/cinematic/tracking/types.py`
- Existing config: `configs/cinematic/tracking/camera_profiles.yaml`

### Secondary (MEDIUM confidence)
- Blender libmv documentation (camera solver uses same distortion model)
- Nuke ST-Map documentation (compositing standard)
- iPhone QuickTime metadata specification (Apple developer docs)

### Tertiary (LOW confidence)
- Web search results for rolling shutter detection algorithms (requires practical validation)

## Metadata

**Confidence breakdown:**
- Standard stack: HIGH - ffprobe and OpenCV are industry standards
- Architecture: HIGH - Patterns align with existing codebase
- Pitfalls: MEDIUM - Based on common issues, may have edge cases

**Research date:** 2026-02-19
**Valid until:** 2026-03-19 (30 days - stable libraries)
