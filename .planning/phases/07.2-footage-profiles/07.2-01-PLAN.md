---
phase: 07.2-footage-profiles
plan: 01
type: execute
wave: 1
depends_on: []
files_modified:
  - lib/cinematic/tracking/footage.py
  - lib/cinematic/tracking/types.py
autonomous: true

must_haves:
  truths:
    - "FFprobe extracts video metadata (resolution, frame rate, codec, duration)"
    - "iPhone QuickTime metadata extraction returns camera model and focal length"
    - "Content analysis returns motion blur, noise, and contrast levels"
  artifacts:
    - path: "lib/cinematic/tracking/footage.py"
      provides: "FFprobeMetadataExtractor, iPhone metadata parsing, content analysis"
      exports: ["FFprobeMetadataExtractor", "FootageMetadata", "analyze_content"]
    - path: "lib/cinematic/tracking/types.py"
      provides: "Extended FootageMetadata dataclass"
      contains: "class FootageMetadata"
  key_links:
    - from: "FFprobeMetadataExtractor"
      to: "FootageMetadata"
      via: "extract() returns FootageMetadata"
      pattern: "def extract.*FootageMetadata"
    - from: "analyze_content()"
      to: "FootageMetadata"
      via: "extract_full() integrates content analysis results"
      pattern: "def extract_full.*FootageMetadata"
---

<objective>
Extend footage.py with comprehensive FFprobeMetadataExtractor, iPhone QuickTime metadata parsing, and content analysis (motion blur, noise, contrast).

Purpose: Provide rich metadata extraction for tracking workflows - device detection, content quality assessment, and footage characteristics.
Output: Extended footage.py with FFprobeMetadataExtractor class and FootageMetadata dataclass.
</objective>

<execution_context>
@/Users/bretbouchard/.claude/get-shit-done/workflows/execute-plan.md
@/Users/bretbouchard/.claude/get-shit-done/templates/summary.md
</execution_context>

<context>
@.planning/PROJECT.md
@.planning/ROADMAP.md
@.planning/STATE.md
@.planning/phases/07.2-footage-profiles/07.2-RESEARCH.md

# Existing code to extend
@lib/cinematic/tracking/footage.py
@lib/cinematic/tracking/types.py
</context>

<tasks>

<task type="auto">
  <name>Task 1: Add FootageMetadata dataclass to types.py</name>
  <files>lib/cinematic/tracking/types.py</files>
  <action>
    Add FootageMetadata dataclass after FootageInfo with comprehensive metadata fields:

    ```python
    @dataclass
    class FootageMetadata:
        """
        Comprehensive footage metadata with content analysis.

        Extends FootageInfo with device metadata and content quality metrics.

        Attributes:
            source_path: Path to source file
            width, height: Resolution in pixels
            fps: Frames per second
            frame_count: Total frames
            duration_seconds: Duration in seconds
            codec: Video codec name
            codec_profile: Codec profile (e.g., "High", "Main")
            bit_depth: Bits per channel (8, 10, 12)
            color_space: Color space (e.g., "bt709", "bt2020")
            color_range: Color range ("limited", "full")
            pixel_aspect_ratio: Pixel aspect ratio
            camera_make: Camera manufacturer (from QuickTime metadata)
            camera_model: Camera model (from QuickTime metadata)
            lens_model: Lens model if available
            focal_length_mm: Focal length in mm
            iso: ISO value
            aperture: Aperture as string (e.g., "f/1.8")
            white_balance: White balance Kelvin
            motion_blur_level: "low", "medium", "high"
            noise_level: "low", "medium", "high"
            contrast_suitability: "poor", "fair", "good", "excellent"
            dominant_motion: "static", "pan", "tilt", "zoom", "handheld"
        """
        source_path: str = ""
        width: int = 1920
        height: int = 1080
        fps: float = 24.0
        frame_count: int = 0
        duration_seconds: float = 0.0
        codec: str = ""
        codec_profile: str = ""
        bit_depth: int = 8
        color_space: str = ""
        color_range: str = "limited"
        pixel_aspect_ratio: float = 1.0
        # Device metadata
        camera_make: str = ""
        camera_model: str = ""
        lens_model: str = ""
        focal_length_mm: float = 0.0
        iso: int = 0
        aperture: str = ""
        white_balance: int = 0
        # Content analysis
        motion_blur_level: str = "medium"
        noise_level: str = "medium"
        contrast_suitability: str = "good"
        dominant_motion: str = "handheld"

        def to_dict(self) -> Dict[str, Any]:
            # ... serialization

        @classmethod
        def from_dict(cls, data: Dict[str, Any]) -> FootageMetadata:
            # ... deserialization
    ```

    Include to_dict() and from_dict() methods following existing patterns in types.py.
  </action>
  <verify>python -c "from lib.cinematic.tracking.types import FootageMetadata; m = FootageMetadata(); print('FootageMetadata OK')" 2>/dev/null || echo "Types check passed (outside Blender)"</verify>
  <done>FootageMetadata dataclass exists with all fields and to_dict/from_dict methods</done>
</task>

<task type="auto">
  <name>Task 2: Add FFprobeMetadataExtractor class to footage.py</name>
  <files>lib/cinematic/tracking/footage.py</files>
  <action>
    Add FFprobeMetadataExtractor class after FootageImporter that provides:

    1. extract() method - Main entry point returning FootageMetadata
    2. _run_ffprobe() - Execute ffprobe CLI with JSON output
    3. _parse_video_stream() - Extract video dimensions, fps, codec
    4. _parse_device_metadata() - Extract iPhone/camera specific metadata from QuickTime atoms
    5. _parse_fps_string() - Handle "30000/1001" fractional frame rates

    Key implementation details:
    - Use subprocess.run() with ffprobe command
    - Handle timeout (30 seconds) and missing ffprobe gracefully
    - Parse format.tags for QuickTime metadata (com.apple.quicktime.model, etc.)
    - Parse r_frame_rate as fraction: "30000/1001" -> 29.97

    ```python
    class FFprobeMetadataExtractor:
        """Extract comprehensive video metadata using ffprobe."""

        def extract(self, source_path: str) -> FootageMetadata:
            """Extract all metadata from video file."""
            data = self._run_ffprobe(source_path)
            if not data:
                return FootageMetadata(source_path=source_path)

            metadata = FootageMetadata(source_path=source_path)
            self._parse_video_stream(data, metadata)
            self._parse_device_metadata(data, metadata)
            return metadata

        def _run_ffprobe(self, source_path: str) -> Optional[Dict]:
            """Run ffprobe and return JSON data."""
            # subprocess.run with ffprobe -show_format -show_streams -show_data
            # Returns None if ffprobe not available

        def _parse_video_stream(self, data: Dict, metadata: FootageMetadata):
            """Parse video stream for resolution, fps, codec."""
            # Find codec_type == "video" stream
            # Parse r_frame_rate as fraction
            # Extract width, height, codec_name, profile, bits_per_raw_sample

        def _parse_device_metadata(self, data: Dict, metadata: FootageMetadata):
            """Parse device-specific QuickTime metadata."""
            # Look for com.apple.quicktime.model, com.apple.quicktime.make
            # Extract focal_length, iso, aperture from stream tags
    ```

    The existing _extract_metadata_ffprobe() method in FootageImporter returns VideoMetadata.
    This new class provides more comprehensive extraction with device metadata.
  </action>
  <verify>grep -q "class FFprobeMetadataExtractor" lib/cinematic/tracking/footage.py && echo "FFprobeMetadataExtractor found"</verify>
  <done>FFprobeMetadataExtractor class exists with extract(), _run_ffprobe(), _parse_video_stream(), _parse_device_metadata() methods</done>
</task>

<task type="auto">
  <name>Task 3: Add content analysis function to footage.py</name>
  <files>lib/cinematic/tracking/footage.py</files>
  <action>
    Add analyze_content() function that samples frames and analyzes:

    1. Motion blur level - Calculate frame-to-frame difference intensity
    2. Noise level - Estimate noise from flat regions
    3. Contrast suitability - Calculate histogram spread
    4. Dominant motion type - Detect camera movement pattern

    IMPORTANT: This function MUST support both video files AND image sequences:
    - For video files (MOV, MP4, MXF, AVI): Use cv2.VideoCapture for frame sampling
    - For image sequences (PNG, JPEG, EXR, DPX): Detect sequence pattern and sample individual frame files

    ```python
    def analyze_content(
        source_path: str,
        sample_frames: int = 30,
        use_opencv: bool = True
    ) -> Dict[str, Any]:
        """
        Analyze footage content quality for tracking suitability.

        Supports both video files and image sequences:
        - Video: MOV, MP4, MXF, AVI (uses cv2.VideoCapture)
        - Image sequences: PNG, JPEG, EXR, DPX (samples individual frame files)

        Args:
            source_path: Path to video file OR image sequence (file pattern or directory)
            sample_frames: Number of frames to sample
            use_opencv: Use OpenCV if available (more accurate)

        Returns:
            Dict with motion_blur_level, noise_level, contrast_suitability, dominant_motion
        """
        # Detect if source is video file or image sequence
        if _is_image_sequence(source_path):
            return _analyze_image_sequence(source_path, sample_frames, use_opencv)

        # Try OpenCV-based analysis for video files
        if use_opencv:
            try:
                import cv2
                return _analyze_content_opencv(source_path, sample_frames)
            except ImportError:
                pass

        # Fallback to basic analysis
        return _analyze_content_basic(source_path, sample_frames)

    def _is_image_sequence(source_path: str) -> bool:
        """Detect if path is an image sequence (directory or frame pattern)."""
        # Check for common patterns: directory with numbered frames, or glob pattern
        # Supports: frame.0001.exr, image_001.png, etc.
        pass

    def _analyze_image_sequence(source_path: str, sample_frames: int, use_opencv: bool) -> Dict[str, Any]:
        """Analyze image sequence by sampling individual frame files."""
        # Detect frame files in directory or from glob pattern
        # Sample N frames evenly across sequence
        # Load each frame and perform same analysis as video
        # Return aggregated results
        pass

    def _analyze_content_opencv(source_path: str, sample_frames: int) -> Dict[str, Any]:
        """OpenCV-based content analysis for video files."""
        # Use cv2.VideoCapture to sample frames
        # Calculate frame differences for motion blur
        # Estimate noise from variance in flat regions
        # Calculate histogram for contrast
        # Detect motion pattern (static, pan, tilt, zoom, handheld)

    def _analyze_content_basic(source_path: str, sample_frames: int) -> Dict[str, Any]:
        """Basic content analysis without OpenCV."""
        # Return default values with warning
        return {
            "motion_blur_level": "medium",
            "noise_level": "medium",
            "contrast_suitability": "good",
            "dominant_motion": "handheld",
        }
    ```

    Use cv2.VideoCapture for video frame sampling. For image sequences:
    - Detect sequence from glob pattern or directory listing
    - Sample individual frame files using cv2.imread()
    - Perform same analysis on sampled frames

    Calculate:
    - Motion blur: Mean absolute difference between consecutive frames
    - Noise: Variance in low-gradient regions
    - Contrast: Standard deviation of luminance histogram
    - Motion: Optical flow direction variance (handheld = high variance)
  </action>
  <verify>grep -q "def analyze_content" lib/cinematic/tracking/footage.py && grep -q "_is_image_sequence\|_analyze_image_sequence" lib/cinematic/tracking/footage.py && echo "analyze_content with image sequence support found"</verify>
  <done>analyze_content() function exists with OpenCV-based analysis, image sequence support, and fallback</done>
</task>

<task type="auto">
  <name>Task 4: Add extract_full() method integrating FFprobe + content analysis</name>
  <files>lib/cinematic/tracking/footage.py</files>
  <action>
    Add an extract_full() method to FFprobeMetadataExtractor that combines FFprobe metadata with content analysis results into a complete FootageMetadata object.

    This creates the key link between:
    - FFprobeMetadataExtractor (Task 2) - extracts video/device metadata
    - analyze_content() (Task 3) - analyzes motion blur, noise, contrast
    - FootageMetadata (Task 1) - the unified dataclass

    ```python
    class FFprobeMetadataExtractor:
        # ... existing methods from Task 2 ...

        def extract_full(
            self,
            source_path: str,
            analyze: bool = True,
            sample_frames: int = 30,
        ) -> FootageMetadata:
            """
            Extract complete metadata including content analysis.

            Combines FFprobe extraction with content analysis to populate
            all FootageMetadata fields.

            Args:
                source_path: Path to video file or image sequence
                analyze: Whether to perform content analysis (default: True)
                sample_frames: Number of frames to sample for analysis

            Returns:
                FootageMetadata with all fields populated
            """
            # Get basic FFprobe metadata
            metadata = self.extract(source_path)

            if analyze:
                # Run content analysis and integrate results
                content = analyze_content(source_path, sample_frames)
                metadata.motion_blur_level = content.get("motion_blur_level", "medium")
                metadata.noise_level = content.get("noise_level", "medium")
                metadata.contrast_suitability = content.get("contrast_suitability", "good")
                metadata.dominant_motion = content.get("dominant_motion", "handheld")

            return metadata
    ```

    This method provides a single-call interface to get complete footage metadata.
  </action>
  <verify>grep -q "def extract_full" lib/cinematic/tracking/footage.py && echo "extract_full integration method found"</verify>
  <done>FFprobeMetadataExtractor.extract_full() combines FFprobe extraction with content analysis into complete FootageMetadata</done>
</task>

</tasks>

<verification>
- FootageMetadata dataclass exists in types.py with all required fields
- FFprobeMetadataExtractor class in footage.py with extract() and extract_full() methods
- analyze_content() function provides motion blur, noise, contrast analysis for video AND image sequences
- extract_full() integrates FFprobe + content analysis into complete FootageMetadata
- All classes follow to_dict/from_dict serialization pattern
- Code runs without errors (outside Blender with try/except guards)
</verification>

<success_criteria>
- FFprobeMetadataExtractor.extract() returns FootageMetadata with device info
- FFprobeMetadataExtractor.extract_full() returns complete FootageMetadata with content analysis
- iPhone QuickTime metadata (model, focal length) extracted correctly
- Content analysis works for both video files (MOV, MP4, MXF, AVI) and image sequences (PNG, JPEG, EXR, DPX)
- Graceful fallback when ffprobe or OpenCV unavailable
</success_criteria>

<output>
After completion, create `.planning/phases/07.2-footage-profiles/07.2-01-SUMMARY.md`
</output>
