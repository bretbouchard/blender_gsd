---
phase: 06.9-camera-matching
plan: 02
type: execute
wave: 2
depends_on: [06.9-01]
files_modified:
  - lib/cinematic/camera_match.py
  - lib/cinematic/audio_sync.py
autonomous: true

must_haves:
  truths:
    - "Reference image matching works"
    - "Tracking data import supported"
    - "Audio loads and markers placed"
  artifacts:
    - path: "lib/cinematic/camera_match.py"
      provides: "Camera matching from reference and tracking import"
      exports: ["match_camera_to_reference", "import_tracking_data", "estimate_focal_length"]
    - path: "lib/cinematic/audio_sync.py"
      provides: "Audio sync and beat marker support"
      exports: ["load_audio", "place_beat_markers", "detect_bpm"]
  key_links:
    - from: "lib/cinematic/camera_match.py"
      to: "lib/cinematic/camera.py"
      via: "camera configuration"
---

<objective>
Create camera_match.py and audio_sync.py modules (REQ-CINE-MATCH, REQ-CINE-AUDIO).

Purpose: Provide camera matching from reference images, tracking data import, and audio sync with beat markers.
Output: camera_match.py and audio_sync.py modules with matching and sync functions.
</objective>

<execution_context>
@/Users/bretbouchard/.claude/get-shit-done/workflows/execute-plan.md
@/Users/bretbouchard/.claude/get-shit-done/templates/summary.md
</execution_context>

<context>
@.planning/PROJECT.md
@.planning/ROADMAP.md
@.planning/STATE.md

# Existing patterns
@lib/cinematic/camera.py
@lib/cinematic/types.py

# Requirements
@.planning/REQUIREMENTS_CINEMATIC.md (REQ-CINE-MATCH, REQ-CINE-AUDIO)
</context>

<tasks>

<task type="auto">
  <name>Task 1: Create camera_match.py module</name>
  <files>lib/cinematic/camera_match.py</files>
  <action>
Create `lib/cinematic/camera_match.py` with camera matching and tracking import functions.

```python
"""
Camera Matching Module (REQ-CINE-MATCH)

Camera matching from reference images and tracking data import.
Supports focal length estimation, horizon matching, and external
tracking data from Nuke, After Effects, FBX, etc.

Usage:
    from lib.cinematic.camera_match import (
        match_camera_to_reference, import_tracking_data, estimate_focal_length
    )

    # Match camera to reference image
    camera = match_camera_to_reference("reference.jpg", subject_bounds=(100, 100, 500, 400))

    # Import tracking data
    camera = import_tracking_data("tracking.fbx", format="fbx")
"""

from __future__ import annotations
from pathlib import Path
from typing import Dict, Any, Optional, List, Tuple
import math

try:
    import bpy
    BLENDER_AVAILABLE = True
except ImportError:
    bpy = None
    BLENDER_AVAILABLE = False

from .types import CameraMatchConfig, TrackingImportConfig, CameraProfile, CameraConfig


def estimate_focal_length(
    image_width: int,
    vanishing_points: List[Tuple[float, float]],
    sensor_width: float = 36.0
) -> float:
    """
    Estimate focal length from vanishing points.

    Uses the perspective projection relationship between vanishing
    points and focal length.

    Args:
        image_width: Image width in pixels
        vanishing_points: List of vanishing point (x, y) coordinates
        sensor_width: Camera sensor width in mm

    Returns:
        Estimated focal length in mm
    """
    if len(vanishing_points) < 2:
        # Not enough data - return default
        return 50.0

    # Calculate distance from image center to vanishing point
    center_x = image_width / 2

    # Use average distance for estimation
    distances = [abs(vp[0] - center_x) for vp in vanishing_points[:2]]
    avg_distance = sum(distances) / len(distances)

    # Focal length estimation from perspective geometry
    # f = (sensor_width * image_width) / (2 * vp_distance)
    if avg_distance > 0:
        # This is a simplified estimation
        focal_length = (sensor_width * image_width) / (2 * avg_distance * 2)
        # Clamp to reasonable range
        focal_length = max(14.0, min(200.0, focal_length))
    else:
        focal_length = 50.0

    return focal_length


def detect_horizon_line(
    image_path: str
) -> float:
    """
    Detect horizon line position in image.

    This is a placeholder - full implementation would use edge detection
    and vanishing point analysis.

    Args:
        image_path: Path to reference image

    Returns:
        Horizon line position (0-1 normalized)
    """
    # Placeholder - return default (middle of image)
    # Full implementation would analyze image for horizon
    return 0.5


def match_camera_to_reference(
    config: CameraMatchConfig,
    camera_name: str = "matched_camera",
    scene: Optional[Any] = None
) -> Optional[Any]:
    """
    Match a camera to a reference image.

    Creates a camera positioned and configured to match the perspective
    of the reference image.

    Args:
        config: Camera match configuration
        camera_name: Name for created camera
        scene: Optional scene (uses context if None)

    Returns:
        Created/modified camera object or None
    """
    if not BLENDER_AVAILABLE:
        return None

    if scene is None:
        scene = bpy.context.scene

    try:
        # Create or get camera
        if camera_name in bpy.data.objects:
            camera_obj = bpy.data.objects[camera_name]
        else:
            cam_data = bpy.data.cameras.new(name=f"{camera_name}_data")
            camera_obj = bpy.data.objects.new(camera_name, cam_data)
            scene.collection.objects.link(camera_obj)

        camera = camera_obj.data

        # Set background image for reference
        if config.reference_image and Path(config.reference_image).exists():
            # Add background image
            if not hasattr(camera, "background_images"):
                return camera_obj

            bg = camera.background_images.new()
            bg.image = bpy.data.images.load(config.reference_image)
            bg.display_depth = 'BACK'

        # Estimate focal length
        if config.auto_detect_focal and config.focal_length_estimate == 0:
            # Use vanishing points if available
            if config.vanishing_points:
                focal = estimate_focal_length(
                    1920,  # Assume 1080p
                    config.vanishing_points,
                    camera.sensor_width
                )
            else:
                focal = 50.0  # Default
        else:
            focal = config.focal_length_estimate if config.focal_length_estimate > 0 else 50.0

        camera.lens = focal

        # Set horizon line (affects camera pitch)
        if config.auto_detect_horizon:
            horizon = detect_horizon_line(config.reference_image)
        else:
            horizon = config.horizon_line

        # Adjust camera pitch based on horizon
        # Horizon at 0.5 = level camera, above = tilted up, below = tilted down
        pitch_offset = (horizon - 0.5) * 0.5  # Max 0.25 radians

        # Position camera at appropriate distance
        camera_obj.location = (0, -3, 1.7)  # Typical viewing position
        camera_obj.rotation_euler = (pitch_offset, 0, 0)

        return camera_obj

    except Exception:
        return None


def import_tracking_data(
    config: TrackingImportConfig,
    camera_name: str = "tracked_camera",
    scene: Optional[Any] = None
) -> Optional[Any]:
    """
    Import tracking data from external file.

    Supports FBX, Alembic, BVH, and custom JSON formats.

    Args:
        config: Tracking import configuration
        camera_name: Name for imported camera
        scene: Optional scene

    Returns:
        Imported camera object or None
    """
    if not BLENDER_AVAILABLE:
        return None

    if scene is None:
        scene = bpy.context.scene

    file_path = Path(config.file_path)
    if not file_path.exists():
        return None

    try:
        if config.format == "fbx":
            return _import_fbx_camera(file_path, camera_name, config, scene)
        elif config.format == "alembic":
            return _import_alembic_camera(file_path, camera_name, config, scene)
        elif config.format == "bvh":
            return _import_bvh_camera(file_path, camera_name, config, scene)
        elif config.format == "json":
            return _import_json_camera(file_path, camera_name, config, scene)
        elif config.format == "nuke_chan":
            return _import_nuke_chan(file_path, camera_name, config, scene)
        else:
            return None
    except Exception:
        return None


def _import_fbx_camera(
    file_path: Path,
    camera_name: str,
    config: TrackingImportConfig,
    scene: Any
) -> Optional[Any]:
    """Import camera from FBX file."""
    # Use Blender's FBX importer
    bpy.ops.import_scene.fbx(filepath=str(file_path))

    # Find imported camera
    for obj in scene.objects:
        if obj.type == "CAMERA":
            obj.name = camera_name
            # Apply coordinate conversion if needed
            if config.coordinate_system == "y_up":
                # Convert Y-up to Z-up (Blender default)
                pass  # Blender handles this in import
            return obj

    return None


def _import_alembic_camera(
    file_path: Path,
    camera_name: str,
    config: TrackingImportConfig,
    scene: Any
) -> Optional[Any]:
    """Import camera from Alembic file."""
    if not hasattr(bpy.ops, "wm"):
        return None

    bpy.ops.wm.alembic_import(filepath=str(file_path))

    for obj in scene.objects:
        if obj.type == "CAMERA":
            obj.name = camera_name
            return obj

    return None


def _import_bvh_camera(
    file_path: Path,
    camera_name: str,
    config: TrackingImportConfig,
    scene: Any
) -> Optional[Any]:
    """Import camera from BVH motion capture file."""
    if not hasattr(bpy.ops, "import_anim"):
        return None

    bpy.ops.import_anim.bvh(filepath=str(file_path))

    # BVH creates armature, need to extract camera from it
    # This is simplified - full implementation would create camera
    # that follows armature motion
    for obj in scene.objects:
        if obj.type == "ARMATURE":
            # Create camera parented to armature
            cam_data = bpy.data.cameras.new(name=f"{camera_name}_data")
            cam_obj = bpy.data.objects.new(camera_name, cam_data)
            scene.collection.objects.link(cam_obj)
            cam_obj.parent = obj
            return cam_obj

    return None


def _import_json_camera(
    file_path: Path,
    camera_name: str,
    config: TrackingImportConfig,
    scene: Any
) -> Optional[Any]:
    """Import camera from custom JSON format."""
    import json

    with open(file_path, "r") as f:
        data = json.load(f)

    # Create camera
    cam_data = bpy.data.cameras.new(name=f"{camera_name}_data")
    cam_obj = bpy.data.objects.new(camera_name, cam_data)
    scene.collection.objects.link(cam_obj)

    # Apply frame data
    frames = data.get("frames", [])
    for frame_data in frames:
        frame = frame_data.get("frame", 1) + config.frame_offset
        position = frame_data.get("position", [0, -3, 0])
        rotation = frame_data.get("rotation", [0, 0, 0])

        cam_obj.location = position
        cam_obj.rotation_euler = [math.radians(r) for r in rotation]

        cam_obj.keyframe_insert(data_path="location", frame=frame)
        cam_obj.keyframe_insert(data_path="rotation_euler", frame=frame)

    # Set lens
    if "focal_length" in data:
        cam_data.lens = data["focal_length"]

    return cam_obj


def _import_nuke_chan(
    file_path: Path,
    camera_name: str,
    config: TrackingImportConfig,
    scene: Any
) -> Optional[Any]:
    """Import camera from Nuke .chan file."""
    # Create camera
    cam_data = bpy.data.cameras.new(name=f"{camera_name}_data")
    cam_obj = bpy.data.objects.new(camera_name, cam_data)
    scene.collection.objects.link(cam_obj)

    # Parse .chan file (tab-separated: frame tx ty tz rx ry rz fov)
    with open(file_path, "r") as f:
        for line in f:
            if line.startswith("#") or not line.strip():
                continue

            parts = line.strip().split()
            if len(parts) >= 7:
                frame = int(float(parts[0])) + config.frame_offset
                tx, ty, tz = float(parts[1]), float(parts[2]), float(parts[3])
                rx, ry, rz = float(parts[4]), float(parts[5]), float(parts[6])

                # Nuke Y-up to Blender Z-up conversion
                if config.coordinate_system == "y_up":
                    ty, tz = tz, -ty

                cam_obj.location = (tx * config.scale_factor, ty * config.scale_factor, tz * config.scale_factor)
                cam_obj.rotation_euler = (math.radians(rx), math.radians(ry), math.radians(rz))

                cam_obj.keyframe_insert(data_path="location", frame=frame)
                cam_obj.keyframe_insert(data_path="rotation_euler", frame=frame)

                if len(parts) >= 8:
                    fov = float(parts[7])
                    # Convert FOV to focal length (simplified)
                    cam_data.lens = 36.0 / (2 * math.tan(math.radians(fov / 2)))

    return cam_obj


def apply_camera_profile(
    camera_obj: Any,
    profile: CameraProfile
) -> None:
    """
    Apply camera device profile to camera.

    Sets sensor size, distortion parameters, and other device-specific settings.

    Args:
        camera_obj: Blender camera object
        profile: Camera profile to apply
    """
    if not BLENDER_AVAILABLE or camera_obj is None:
        return

    camera = camera_obj.data

    # Set sensor dimensions
    camera.sensor_width = profile.sensor_width
    camera.sensor_height = profile.sensor_height
    camera.lens = profile.focal_length

    # Note: Full distortion implementation would require compositor nodes
    # or lens distortion modifier (not available in base Blender)
```
  </action>
  <verify>python3 -c "
from lib.cinematic.camera_match import match_camera_to_reference, import_tracking_data, estimate_focal_length, apply_camera_profile
print('camera_match module imports OK')
"</verify>
  <done>camera_match.py with matching and tracking import functions</done>
</task>

<task type="auto">
  <name>Task 2: Create audio_sync.py module</name>
  <files>lib/cinematic/audio_sync.py</files>
  <action>
Create `lib/cinematic/audio_sync.py` with audio loading and beat marker functions.

```python
"""
Audio Sync Module (REQ-CINE-AUDIO)

Audio track support for animation timing and synchronization.
Supports loading audio files, placing beat markers, and BPM detection.

Usage:
    from lib.cinematic.audio_sync import load_audio, place_beat_markers, detect_bpm

    # Load audio file
    load_audio("soundtrack.wav")

    # Place beat markers
    place_beat_markers(bpm=120, fps=24)

    # Auto-detect BPM
    bpm = detect_bpm("music.mp3")
"""

from __future__ import annotations
from pathlib import Path
from typing import Dict, Any, Optional, List
import math

try:
    import bpy
    BLENDER_AVAILABLE = True
except ImportError:
    bpy = None
    BLENDER_AVAILABLE = False

from .types import AudioSyncConfig


def load_audio(
    audio_file: str,
    scene: Optional[Any] = None,
    offset_frames: int = 0
) -> bool:
    """
    Load audio file into scene.

    Adds audio to the VSE (Video Sequence Editor) for playback
    and waveform visualization.

    Args:
        audio_file: Path to audio file
        scene: Optional scene (uses context if None)
        offset_frames: Frame offset for audio start

    Returns:
        True on success
    """
    if not BLENDER_AVAILABLE:
        return False

    if scene is None:
        scene = bpy.context.scene

    path = Path(audio_file)
    if not path.exists():
        return False

    try:
        # Ensure scene has VSE data
        if not scene.sequence_editor:
            scene.sequence_editor_create()

        seq = scene.sequence_editor

        # Check if audio already loaded
        for strip in seq.sequences:
            if strip.type == "SOUND" and strip.sound:
                if strip.sound.filepath == str(path.absolute()):
                    return True

        # Load audio
        sound = bpy.data.sounds.load(str(path.absolute()))
        strip = seq.sequences.new_sound(
            name=path.stem,
            sound=sound,
            channel=1,
            frame_start=scene.frame_start + offset_frames
        )

        return True

    except Exception:
        return False


def place_beat_markers(
    bpm: float,
    fps: float = 24.0,
    start_frame: int = 1,
    duration_frames: int = 250,
    scene: Optional[Any] = None,
    marker_prefix: str = "Beat"
) -> List[int]:
    """
    Place timeline markers on beat intervals.

    Args:
        bpm: Beats per minute
        fps: Frames per second
        start_frame: First frame for markers
        duration_frames: Total duration to place markers
        scene: Optional scene
        marker_prefix: Prefix for marker names

    Returns:
        List of frame numbers where markers were placed
    """
    if not BLENDER_AVAILABLE:
        return []

    if scene is None:
        scene = bpy.context.scene

    if bpm <= 0:
        return []

    # Calculate frames per beat
    beats_per_second = bpm / 60.0
    frames_per_beat = fps / beats_per_second

    marker_frames = []
    beat = 0

    frame = start_frame
    while frame <= start_frame + duration_frames:
        # Create marker
        marker_name = f"{marker_prefix}_{beat:03d}"

        # Add marker to timeline
        if not hasattr(scene, "timeline_markers"):
            return marker_frames

        # Check if marker exists
        existing = None
        for m in scene.timeline_markers:
            if m.frame == frame:
                existing = m
                break

        if existing:
            existing.name = marker_name
        else:
            marker = scene.timeline_markers.new(marker_name, frame=frame)

        marker_frames.append(int(frame))
        beat += 1
        frame = start_frame + (beat * frames_per_beat)

    return marker_frames


def detect_bpm(
    audio_file: str
) -> float:
    """
    Detect BPM from audio file.

    This is a placeholder - full implementation would use audio analysis
    libraries like librosa or aubio.

    Args:
        audio_file: Path to audio file

    Returns:
        Detected BPM or 0 if detection fails
    """
    # Placeholder - returns 0 (auto-detect not implemented)
    # Full implementation would:
    # 1. Load audio samples
    # 2. Compute onset strength
    # 3. Find tempo via autocorrelation
    # 4. Return most likely BPM

    return 0.0


def create_animation_markers(
    config: AudioSyncConfig,
    scene: Optional[Any] = None
) -> List[int]:
    """
    Create timeline markers from audio sync configuration.

    Combines audio loading with beat marker placement.

    Args:
        config: Audio sync configuration
        scene: Optional scene

    Returns:
        List of created marker frame numbers
    """
    if not BLENDER_AVAILABLE:
        return []

    if scene is None:
        scene = bpy.context.scene

    marker_frames = []

    # Load audio if specified
    if config.audio_file:
        load_audio(config.audio_file, scene, config.offset_frames)

    # Determine BPM
    bpm = config.bpm
    if bpm <= 0 and config.auto_detect_bpm:
        bpm = detect_bpm(config.audio_file)

    if bpm <= 0:
        bpm = 120.0  # Default fallback

    # Place beat markers
    if config.beat_markers:
        # Use explicit beat frames
        for i, frame in enumerate(config.beat_markers):
            frame = frame + config.offset_frames
            marker_name = f"Beat_{i:03d}"
            if hasattr(scene, "timeline_markers"):
                scene.timeline_markers.new(marker_name, frame=frame)
            marker_frames.append(frame)
    else:
        # Auto-generate from BPM
        fps = scene.render.fps / scene.render.fps_base
        marker_frames = place_beat_markers(
            bpm=bpm,
            fps=fps,
            start_frame=scene.frame_start,
            duration_frames=scene.frame_end - scene.frame_start,
            scene=scene
        )

    # Add named markers
    if config.markers:
        for name, frame in config.markers.items():
            frame = frame + config.offset_frames
            if hasattr(scene, "timeline_markers"):
                scene.timeline_markers.new(name, frame=frame)
            marker_frames.append(frame)

    return marker_frames


def get_frame_at_beat(
    beat_number: int,
    bpm: float,
    fps: float = 24.0,
    start_frame: int = 1
) -> int:
    """
    Calculate frame number for a specific beat.

    Args:
        beat_number: Beat number (0-indexed)
        bpm: Beats per minute
        fps: Frames per second
        start_frame: First frame

    Returns:
        Frame number for the beat
    """
    if bpm <= 0:
        return start_frame

    beats_per_second = bpm / 60.0
    frames_per_beat = fps / beats_per_second

    return int(start_frame + (beat_number * frames_per_beat))


def get_beat_at_frame(
    frame: int,
    bpm: float,
    fps: float = 24.0,
    start_frame: int = 1
) -> int:
    """
    Calculate beat number for a specific frame.

    Args:
        frame: Frame number
        bpm: Beats per minute
        fps: Frames per second
        start_frame: First frame

    Returns:
        Beat number at frame
    """
    if bpm <= 0:
        return 0

    beats_per_second = bpm / 60.0
    frames_per_beat = fps / beats_per_second

    return int((frame - start_frame) / frames_per_beat)
```
  </action>
  <verify>python3 -c "
from lib.cinematic.audio_sync import load_audio, place_beat_markers, detect_bpm, create_animation_markers, get_frame_at_beat, get_beat_at_frame
print('audio_sync module imports OK')
"</verify>
  <done>audio_sync.py with audio loading and beat marker functions</done>
</task>

</tasks>

<verification>
1. camera_match.py imports without errors
2. audio_sync.py imports without errors
3. All functions exist and are callable
</verification>

<success_criteria>
- camera_match.py with match_camera_to_reference, import_tracking_data
- audio_sync.py with load_audio, place_beat_markers
- Pattern matches existing modules
</success_criteria>

<output>
After completion, create `.planning/phases/06.9-camera-matching/06.9-02-SUMMARY.md`
</output>
